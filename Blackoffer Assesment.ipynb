{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cee537a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\rahul\\anaconda3\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy) (63.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy) (1.10.7)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy) (8.1.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bca34549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting syllables\n",
      "  Downloading syllables-1.0.7-py3-none-any.whl (15 kB)\n",
      "Collecting importlib-metadata<6.0.0,>=5.1.0\n",
      "  Downloading importlib_metadata-5.2.0-py3-none-any.whl (21 kB)\n",
      "Collecting cmudict<2.0.0,>=1.0.11\n",
      "  Downloading cmudict-1.0.13-py3-none-any.whl (939 kB)\n",
      "     -------------------------------------- 939.3/939.3 kB 7.4 MB/s eta 0:00:00\n",
      "Collecting importlib-resources<6.0.0,>=5.10.1\n",
      "  Using cached importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from importlib-metadata<6.0.0,>=5.1.0->syllables) (3.8.0)\n",
      "Installing collected packages: importlib-resources, importlib-metadata, cmudict, syllables\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.11.3\n",
      "    Uninstalling importlib-metadata-4.11.3:\n",
      "      Successfully uninstalled importlib-metadata-4.11.3\n",
      "Successfully installed cmudict-1.0.13 importlib-metadata-5.2.0 importlib-resources-5.12.0 syllables-1.0.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c60462ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.5.0/en_core_web_md-3.5.0-py3-none-any.whl (42.8 MB)\n",
      "     ---------------------------------------- 42.8/42.8 MB 3.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from en-core-web-md==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.11.3)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (63.4.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (21.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.21.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.1)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d4e6188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\rahul\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
      "Requirement already satisfied: click in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f73ee5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import requests\n",
    "import string\n",
    "import spacy\n",
    "import os\n",
    "import nltk\n",
    "# download necessary nltk data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')   \n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "ff52324a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>146</td>\n",
       "      <td>https://insights.blackcoffer.com/blockchain-fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>147</td>\n",
       "      <td>https://insights.blackcoffer.com/the-future-of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>148</td>\n",
       "      <td>https://insights.blackcoffer.com/big-data-anal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>149</td>\n",
       "      <td>https://insights.blackcoffer.com/business-anal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>150</td>\n",
       "      <td>https://insights.blackcoffer.com/challenges-an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID                                                URL\n",
       "0        37  https://insights.blackcoffer.com/ai-in-healthc...\n",
       "1        38  https://insights.blackcoffer.com/what-if-the-c...\n",
       "2        39  https://insights.blackcoffer.com/what-jobs-wil...\n",
       "3        40  https://insights.blackcoffer.com/will-machine-...\n",
       "4        41  https://insights.blackcoffer.com/will-ai-repla...\n",
       "..      ...                                                ...\n",
       "109     146  https://insights.blackcoffer.com/blockchain-fo...\n",
       "110     147  https://insights.blackcoffer.com/the-future-of...\n",
       "111     148  https://insights.blackcoffer.com/big-data-anal...\n",
       "112     149  https://insights.blackcoffer.com/business-anal...\n",
       "113     150  https://insights.blackcoffer.com/challenges-an...\n",
       "\n",
       "[114 rows x 2 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the input.xlsx file\n",
    "data = pd.read_excel(\"C:\\\\Users\\\\Rahul\\\\Downloads\\\\Input.xlsx\")\n",
    "data# Create a folder for all the files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0d0a36",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "be7d9a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder for all the files\n",
    "os.makedirs('data', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "89292976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IntroductionIf anything kills over 10 million...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Human minds, a fascination in itself carrying ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IntroductionAI is rapidly evolving in the empl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anything that could give rise to smarter-than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Machine intelligence is the last invention th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Reconciling with the financial realities of an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>What Is an Investment?An investment is a resou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Quality and affordable healthcare is a vision ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Analytics is a statistical scientific process ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Big DataTo begin with I shall first like to ex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text\n",
       "0    IntroductionIf anything kills over 10 million...\n",
       "1    Human minds, a fascination in itself carrying ...\n",
       "2    IntroductionAI is rapidly evolving in the empl...\n",
       "3    Anything that could give rise to smarter-than...\n",
       "4    Machine intelligence is the last invention th...\n",
       "..                                                 ...\n",
       "109  Reconciling with the financial realities of an...\n",
       "110  What Is an Investment?An investment is a resou...\n",
       "111  Quality and affordable healthcare is a vision ...\n",
       "112  Analytics is a statistical scientific process ...\n",
       "113  Big DataTo begin with I shall first like to ex...\n",
       "\n",
       "[114 rows x 1 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import NavigableString\n",
    "import pandas as pd\n",
    "\n",
    "l = []\n",
    "for url in data['URL']:\n",
    "    title = url.split('/')[3]\n",
    "    file_name = title + '.html'\n",
    "    file_path = './data/' + file_name \n",
    "    #to scrape the data from the url and save it to the file if the file doesnt exist in the directory\n",
    "    if not os.path.exists(file_path):\n",
    "        #print('file doesnt exists')\n",
    "        with open(file_path,'w') as C:\n",
    "            r = requests.get(url, headers={\"User-Agent\": \"XY\"})\n",
    "            htmlcontent = r.content.decode()\n",
    "            C.write(htmlcontent)\n",
    "    #to open each file and retrieve the beautifulsoup from the htmlcontent and put it in a dataframe\n",
    "    with open(file_path, 'r', encoding='ISO-8859-1') as C:  \n",
    "        htmlcontent = C.read()\n",
    "    soup = BeautifulSoup(htmlcontent, 'html.parser')\n",
    "    title = soup.title\n",
    "    #article_Title=soup.find('div', attrs={'class': 'td-post-header'})\n",
    "    article_content = soup.find('div', attrs={'class': 'td-post-content'})\n",
    "    if article_content is None:\n",
    "        article_text = None\n",
    "    else:\n",
    "        article_text = ''\n",
    "        for element in article_content:\n",
    "            if not isinstance(element, NavigableString):\n",
    "                text = element.text\n",
    "                article_text += text\n",
    "    l.append(article_text)\n",
    "\n",
    "df = pd.DataFrame(l, columns=['Text'])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e515740b",
   "metadata": {},
   "source": [
    "### This code does the following:\n",
    "\n",
    "* Uses the \"requests\" module to get the HTML content from the URL.\n",
    "* Uses \"BeautifulSoup\" to parse the HTML content.\n",
    "* Gets the title of the page from the \"title\" tag.\n",
    "* Constructs a file name from the title and saves the HTML content to a file in the \"data\" directory.\n",
    "* Opens the file and reads the HTML content.\n",
    "* Finds the \"div\" tag with class \"td-post-content\" and extracts the article content from it.\n",
    "* Appends the article content to a list.\n",
    "* Constructs a DataFrame from the list and returns it.\n",
    "#### Note that the code also handles exceptions when the URL is invalid or when the article content is not found in the HTML content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbb39d0",
   "metadata": {},
   "source": [
    "## checking for null nvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "8a9665de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text    12\n",
       "dtype: int64"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f75251a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Text\n",
       "5    None\n",
       "7    None\n",
       "11   None\n",
       "20   None\n",
       "47   None\n",
       "50   None\n",
       "59   None\n",
       "80   None\n",
       "83   None\n",
       "103  None\n",
       "105  None\n",
       "107  None"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().any(axis=1)\n",
    "df.loc[null_rows]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "232d0ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_drop = [5, 7, 11, 20, 47, 50, 59, 80, 83, 103, 105, 107]\n",
    "clean_df= data.drop(index=rows_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "99e11ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>146</td>\n",
       "      <td>https://insights.blackcoffer.com/blockchain-fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>147</td>\n",
       "      <td>https://insights.blackcoffer.com/the-future-of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>148</td>\n",
       "      <td>https://insights.blackcoffer.com/big-data-anal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>149</td>\n",
       "      <td>https://insights.blackcoffer.com/business-anal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>150</td>\n",
       "      <td>https://insights.blackcoffer.com/challenges-an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID                                                URL\n",
       "0        37  https://insights.blackcoffer.com/ai-in-healthc...\n",
       "1        38  https://insights.blackcoffer.com/what-if-the-c...\n",
       "2        39  https://insights.blackcoffer.com/what-jobs-wil...\n",
       "3        40  https://insights.blackcoffer.com/will-machine-...\n",
       "4        41  https://insights.blackcoffer.com/will-ai-repla...\n",
       "..      ...                                                ...\n",
       "109     146  https://insights.blackcoffer.com/blockchain-fo...\n",
       "110     147  https://insights.blackcoffer.com/the-future-of...\n",
       "111     148  https://insights.blackcoffer.com/big-data-anal...\n",
       "112     149  https://insights.blackcoffer.com/business-anal...\n",
       "113     150  https://insights.blackcoffer.com/challenges-an...\n",
       "\n",
       "[102 rows x 2 columns]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "94554e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 1)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "e90beac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean= df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "6a57f406",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 1)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "48750304",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e029d8d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>146</td>\n",
       "      <td>https://insights.blackcoffer.com/blockchain-fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>147</td>\n",
       "      <td>https://insights.blackcoffer.com/the-future-of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>148</td>\n",
       "      <td>https://insights.blackcoffer.com/big-data-anal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>149</td>\n",
       "      <td>https://insights.blackcoffer.com/business-anal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>150</td>\n",
       "      <td>https://insights.blackcoffer.com/challenges-an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID                                                URL\n",
       "0        37  https://insights.blackcoffer.com/ai-in-healthc...\n",
       "1        38  https://insights.blackcoffer.com/what-if-the-c...\n",
       "2        39  https://insights.blackcoffer.com/what-jobs-wil...\n",
       "3        40  https://insights.blackcoffer.com/will-machine-...\n",
       "4        41  https://insights.blackcoffer.com/will-ai-repla...\n",
       "..      ...                                                ...\n",
       "109     146  https://insights.blackcoffer.com/blockchain-fo...\n",
       "110     147  https://insights.blackcoffer.com/the-future-of...\n",
       "111     148  https://insights.blackcoffer.com/big-data-anal...\n",
       "112     149  https://insights.blackcoffer.com/business-anal...\n",
       "113     150  https://insights.blackcoffer.com/challenges-an...\n",
       "\n",
       "[102 rows x 2 columns]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0464f1ae",
   "metadata": {},
   "source": [
    "## Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "ab83494b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "processed_text = []\n",
    "for text in clean['Text']:\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    # Join the filtered tokens to form a sentence\n",
    "    processed_text.append(' '.join(filtered_tokens))\n",
    "\n",
    "clean['Processed Text'] = processed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "100cab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create stemmer and lemmatizer objects\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# define function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # remove punctuation and convert to lowercase\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "    # tokenize text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    # perform stemming\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    # perform lemmatization\n",
    "    lemma_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # join tokens back into a string\n",
    "    stemmed_text = ' '.join(stemmed_tokens)\n",
    "    lemma_text = ' '.join(lemma_tokens)\n",
    "    return stemmed_text, lemma_text\n",
    "\n",
    "# apply preprocess_text function to text column and create new columns for stemmed and lemmatized text\n",
    "clean['stemmed_text'], clean['lemma_text'] = zip(*clean['Text'].apply(preprocess_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "a6d34e0e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Processed Text</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>lemma_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IntroductionIf anything kills over 10 million...</td>\n",
       "      <td>IntroductionIf anything kills 10 million peop...</td>\n",
       "      <td>introductionif anyth kill 10 million peopl ne...</td>\n",
       "      <td>introductionif anything kill 10 million peopl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Human minds, a fascination in itself carrying ...</td>\n",
       "      <td>Human minds , fascination carrying potential t...</td>\n",
       "      <td>human mind fascin carri potenti tinker natur p...</td>\n",
       "      <td>human mind fascination carrying potential tink...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IntroductionAI is rapidly evolving in the empl...</td>\n",
       "      <td>IntroductionAI rapidly evolving employment sec...</td>\n",
       "      <td>introductionai rapidli evolv employ sector par...</td>\n",
       "      <td>introductionai rapidly evolving employment sec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anything that could give rise to smarter-than...</td>\n",
       "      <td>Anything could give rise smarter-than-human i...</td>\n",
       "      <td>anyth could give rise smarterthanhuman intell...</td>\n",
       "      <td>anything could give rise smarterthanhuman int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Machine intelligence is the last invention th...</td>\n",
       "      <td>Machine intelligence last invention humanity ...</td>\n",
       "      <td>machin intellig last invent human ever need m...</td>\n",
       "      <td>machine intelligence last invention humanity ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Reconciling with the financial realities of an...</td>\n",
       "      <td>Reconciling financial realities MBA education ...</td>\n",
       "      <td>reconcil financi realiti mba educ may come lat...</td>\n",
       "      <td>reconciling financial reality mba education ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>What Is an Investment?An investment is a resou...</td>\n",
       "      <td>Investment ? investment resource thing procure...</td>\n",
       "      <td>investmentan invest resourc thing procur objec...</td>\n",
       "      <td>investmentan investment resource thing procure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Quality and affordable healthcare is a vision ...</td>\n",
       "      <td>Quality affordable healthcare vision governmen...</td>\n",
       "      <td>qualiti afford healthcar vision govern across ...</td>\n",
       "      <td>quality affordable healthcare vision governmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Analytics is a statistical scientific process ...</td>\n",
       "      <td>Analytics statistical scientific process disco...</td>\n",
       "      <td>analyt statist scientif process discov present...</td>\n",
       "      <td>analytics statistical scientific process disco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Big DataTo begin with I shall first like to ex...</td>\n",
       "      <td>Big DataTo begin shall first like explain big ...</td>\n",
       "      <td>big datato begin shall first like explain big ...</td>\n",
       "      <td>big datato begin shall first like explain big ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  \\\n",
       "0    IntroductionIf anything kills over 10 million...   \n",
       "1    Human minds, a fascination in itself carrying ...   \n",
       "2    IntroductionAI is rapidly evolving in the empl...   \n",
       "3    Anything that could give rise to smarter-than...   \n",
       "4    Machine intelligence is the last invention th...   \n",
       "..                                                 ...   \n",
       "109  Reconciling with the financial realities of an...   \n",
       "110  What Is an Investment?An investment is a resou...   \n",
       "111  Quality and affordable healthcare is a vision ...   \n",
       "112  Analytics is a statistical scientific process ...   \n",
       "113  Big DataTo begin with I shall first like to ex...   \n",
       "\n",
       "                                        Processed Text  \\\n",
       "0    IntroductionIf anything kills 10 million peop...   \n",
       "1    Human minds , fascination carrying potential t...   \n",
       "2    IntroductionAI rapidly evolving employment sec...   \n",
       "3    Anything could give rise smarter-than-human i...   \n",
       "4    Machine intelligence last invention humanity ...   \n",
       "..                                                 ...   \n",
       "109  Reconciling financial realities MBA education ...   \n",
       "110  Investment ? investment resource thing procure...   \n",
       "111  Quality affordable healthcare vision governmen...   \n",
       "112  Analytics statistical scientific process disco...   \n",
       "113  Big DataTo begin shall first like explain big ...   \n",
       "\n",
       "                                          stemmed_text  \\\n",
       "0    introductionif anyth kill 10 million peopl ne...   \n",
       "1    human mind fascin carri potenti tinker natur p...   \n",
       "2    introductionai rapidli evolv employ sector par...   \n",
       "3    anyth could give rise smarterthanhuman intell...   \n",
       "4    machin intellig last invent human ever need m...   \n",
       "..                                                 ...   \n",
       "109  reconcil financi realiti mba educ may come lat...   \n",
       "110  investmentan invest resourc thing procur objec...   \n",
       "111  qualiti afford healthcar vision govern across ...   \n",
       "112  analyt statist scientif process discov present...   \n",
       "113  big datato begin shall first like explain big ...   \n",
       "\n",
       "                                            lemma_text  \n",
       "0    introductionif anything kill 10 million peopl...  \n",
       "1    human mind fascination carrying potential tink...  \n",
       "2    introductionai rapidly evolving employment sec...  \n",
       "3    anything could give rise smarterthanhuman int...  \n",
       "4    machine intelligence last invention humanity ...  \n",
       "..                                                 ...  \n",
       "109  reconciling financial reality mba education ma...  \n",
       "110  investmentan investment resource thing procure...  \n",
       "111  quality affordable healthcare vision governmen...  \n",
       "112  analytics statistical scientific process disco...  \n",
       "113  big datato begin shall first like explain big ...  \n",
       "\n",
       "[102 rows x 4 columns]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4c0de7",
   "metadata": {},
   "source": [
    "### POSITIVE SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "9afb88e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Calculate the positive score for each article\n",
    "positive_scores = []\n",
    "for text in clean['Processed Text']:\n",
    "    # Calculate the sentiment scores using VADER\n",
    "    sentiment_scores = analyzer.polarity_scores(text)\n",
    "    # Extract the positive score\n",
    "    positive_score = sentiment_scores['pos']\n",
    "    positive_scores.append(positive_score)\n",
    "\n",
    "# Add the positive scores to the DataFrame\n",
    "clean_df['POSITIVE SCORE'] = positive_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9211f3b",
   "metadata": {},
   "source": [
    "### NEGATIVE SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "0ca17901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the negative score for each article\n",
    "negative_scores = []\n",
    "for text in clean['Processed Text']:\n",
    "    # Calculate the sentiment scores using VADER\n",
    "    sentiment_scores = analyzer.polarity_scores(text)\n",
    "    # Extract the negative score\n",
    "    negative_score = sentiment_scores['neg']\n",
    "    negative_scores.append(negative_score)\n",
    "\n",
    "# Add the negative scores to the DataFrame\n",
    "clean_df['NEGATIVE SCORE'] = negative_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca6fb95",
   "metadata": {},
   "source": [
    "### POLARITY SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "d21c60d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the polarity score for each article\n",
    "polarity_scores = []\n",
    "for text in clean['Processed Text']:\n",
    "    # Calculate the sentiment scores using VADER\n",
    "    sentiment_scores = analyzer.polarity_scores(text)\n",
    "    # Extract the positive and negative scores\n",
    "    positive_score = sentiment_scores['pos']\n",
    "    negative_score = sentiment_scores['neg']\n",
    "    # Calculate the polarity score by subtracting the negative score from the positive score\n",
    "    polarity_score = positive_score - negative_score\n",
    "    polarity_scores.append(polarity_score)\n",
    "\n",
    "# Add the polarity scores to the DataFrame\n",
    "clean_df['POLARITY SCORE'] = polarity_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcdc446",
   "metadata": {},
   "source": [
    "### SUBJECTIVITY SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "d50618b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectivity_scores = []\n",
    "for text in clean['Processed Text']:\n",
    "    # Calculate the TextBlob sentiment scores\n",
    "    sentiment_scores = TextBlob(text).sentiment\n",
    "    # Extract the subjectivity score\n",
    "    subjectivity_score = sentiment_scores.subjectivity\n",
    "    subjectivity_scores.append(subjectivity_score)\n",
    "\n",
    "# Add the subjectivity scores to the DataFrame\n",
    "clean_df['SUBJECTIVITY SCORE'] = subjectivity_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e293a30",
   "metadata": {},
   "source": [
    "### AVG SENTENCE LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "24c2da4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average sentence length for each article\n",
    "avg_sentence_lengths = []\n",
    "for text in clean['Processed Text']:\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    # Count the number of words in each sentence and calculate the average\n",
    "    total_words = sum(len(nltk.word_tokenize(sentence)) for sentence in sentences)\n",
    "    avg_sentence_length = total_words / len(sentences)\n",
    "    avg_sentence_lengths.append(avg_sentence_length)\n",
    "\n",
    "# Add the average sentence lengths to the DataFrame\n",
    "clean_df['AVG SENTENCE LENGTH'] = avg_sentence_lengths"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbf86fb7",
   "metadata": {},
   "source": [
    "# Define a function to calculate the percentage of complex words\n",
    "def calculate_complex_word_percentage(text):\n",
    "    # Tokenize the text into individual words\n",
    "    words = word_tokenize(text)\n",
    "    # Count the number of words with more than 2 syllables\n",
    "    num_complex_words = 0\n",
    "    for word in words:\n",
    "        if len(word) > 2 and nltk.corpus.cmudict.dict().get(word.lower()):\n",
    "            num_complex_words += 1\n",
    "    # Calculate the percentage of complex words\n",
    "    complex_word_percentage = (num_complex_words / len(words)) * 100\n",
    "    return complex_word_percentage\n",
    "\n",
    "# Calculate the percentage of complex words for each article\n",
    "complex_word_percentages = []\n",
    "for text in df['Processed Text']:\n",
    "    complex_word_percentage = calculate_complex_word_percentage(text)\n",
    "    complex_word_percentages.append(complex_word_percentage)\n",
    "\n",
    "# Add the complex word percentages to the DataFrame\n",
    "df['Complex Word Percentage'] = complex_word_percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770304d2",
   "metadata": {},
   "source": [
    "### PERCENTAGE OF COMPLEX WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "4f104a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syllables\n",
    "\n",
    "def count_syllables(word):\n",
    "    return syllables.estimate(word)\n",
    "\n",
    "def calculate_complex_word_percentage(text):\n",
    "    # Tokenize the text into individual words\n",
    "    words = word_tokenize(text)\n",
    "    # Count the number of words with more than 2 syllables\n",
    "    num_complex_words = 0\n",
    "    for word in words:\n",
    "        if count_syllables(word) > 2:\n",
    "            num_complex_words += 1\n",
    "    # Calculate the percentage of complex words\n",
    "    complex_word_percentage = (num_complex_words / len(words)) * 100\n",
    "    return complex_word_percentage\n",
    "\n",
    "# Calculate the percentage of complex words for each article\n",
    "complex_word_percentages = []\n",
    "for text in clean['Processed Text']:\n",
    "    complex_word_percentage = calculate_complex_word_percentage(text)\n",
    "    complex_word_percentages.append(complex_word_percentage)\n",
    "    \n",
    "# Add the complex word percentages to the DataFrame\n",
    "clean_df['PERCENTAGE OF COMPLEX WORDS'] = complex_word_percentages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0e907c",
   "metadata": {},
   "source": [
    "### FOG INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "5a46c89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the FOG Index for a given text\n",
    "def calculate_fog_index(text):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    # Calculate the average sentence length\n",
    "    sentence_lengths = [len(word_tokenize(sentence)) for sentence in sentences]\n",
    "    avg_sentence_length = sum(sentence_lengths) / len(sentence_lengths)\n",
    "    # Calculate the percentage of complex words\n",
    "    stopwords_list = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text.lower())\n",
    "    complex_words = [word for word in words if len(word) > 2 and word not in stopwords_list]\n",
    "    percentage_complex_words = len(complex_words) / len(words)\n",
    "    # Calculate the FOG Index\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    return fog_index\n",
    "\n",
    "# Calculate the FOG Index for each article\n",
    "fog_scores = []\n",
    "for text in clean['Processed Text']:\n",
    "    fog_score = calculate_fog_index(text)\n",
    "    fog_scores.append(fog_score)\n",
    "\n",
    "# Add the FOG Index scores to the DataFrame\n",
    "clean_df['FOG INDEX'] = fog_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1848e3",
   "metadata": {},
   "source": [
    "### AVG NUMBER OF WORDS PER SENTENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "aea38001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average number of words per sentence for each article\n",
    "avg_words_per_sentence = []\n",
    "for text in clean['Text']:\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    # Calculate the average number of words per sentence\n",
    "    num_words = sum(len(nltk.word_tokenize(sentence)) for sentence in sentences)\n",
    "    avg_words_per_sentence.append(num_words / len(sentences))\n",
    "\n",
    "# Add the average number of words per sentence to the DataFrame\n",
    "clean_df['AVG NUMBER OF WORDS PER SENTENCE'] = avg_words_per_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913f1531",
   "metadata": {},
   "source": [
    "### COMPLEX WORD COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "3b48f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a word is a complex word\n",
    "def is_complex_word(word):\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if len(synsets) == 0:\n",
    "        return False\n",
    "    else:\n",
    "        synonyms = set()\n",
    "        for synset in synsets:\n",
    "            synonyms.update([lemma.name() for lemma in synset.lemmas()])\n",
    "        if len(synonyms) > 1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "# Calculate the complex word count for each article\n",
    "complex_word_counts = []\n",
    "for text in clean['Processed Text']:\n",
    "    # Split the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    # Count the number of complex words\n",
    "    complex_word_count = sum([1 for word in words if is_complex_word(word)])\n",
    "    complex_word_counts.append(complex_word_count)\n",
    "\n",
    "# Add the complex word counts to the DataFrame\n",
    "clean_df['COMPLEX WORD COUNT'] = complex_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9056aef8",
   "metadata": {},
   "source": [
    "### WORD COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "4025e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = []\n",
    "for text in clean['Processed Text']:\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    # Count the number of words\n",
    "    count = len(words)\n",
    "    word_counts.append(count)\n",
    "\n",
    "# Add the word counts to the DataFrame\n",
    "clean_df['WORD COUNT'] = word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06f927d",
   "metadata": {},
   "source": [
    "### SYLLABLE PER WORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "bf0f903d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\Rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "nltk.download('cmudict')\n",
    "\n",
    "# Initialize the CMU pronunciation dictionary\n",
    "cmu_dict = cmudict.dict()\n",
    "\n",
    "# Function to count the number of syllables in a word\n",
    "def count_syllables(word):\n",
    "    # Use the CMU pronunciation dictionary to count the number of syllables\n",
    "    if word.lower() in cmu_dict:\n",
    "        return len([syl for syl in cmu_dict[word.lower()][0] if str.isdigit(syl[-1])])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Calculate the average number of syllables per word for each article\n",
    "syllables_per_word = []\n",
    "for text in clean['Processed Text']:\n",
    "    words = text.split()\n",
    "    # Calculate the total number of syllables in the article\n",
    "    total_syllables = sum([count_syllables(word) for word in words])\n",
    "    # Calculate the total number of words in the article\n",
    "    total_words = len(words)\n",
    "    # Calculate the average number of syllables per word\n",
    "    if total_words > 0:\n",
    "        avg_syllables_per_word = total_syllables / total_words\n",
    "    else:\n",
    "        avg_syllables_per_word = 0\n",
    "    syllables_per_word.append(avg_syllables_per_word)\n",
    "\n",
    "# Add the syllables per word to the DataFrame\n",
    "clean_df['SYLLABLE PER WORD'] = syllables_per_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a0e07b",
   "metadata": {},
   "source": [
    "### PERSONAL PRONOUNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "19f9bd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the necessary NLTK data\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Define a function to count personal pronouns\n",
    "def count_personal_pronouns(text):\n",
    "    \"\"\"\n",
    "    This function takes a string of text and returns the count of personal pronouns (first, second, and third person).\n",
    "    \"\"\"\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    # Tag the words with part-of-speech tags\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    # Count the personal pronouns\n",
    "    personal_pronoun_count = 0\n",
    "    for word, tag in tagged_words:\n",
    "        if tag == 'PRP' or tag == 'PRP$':\n",
    "            personal_pronoun_count += 1\n",
    "    return personal_pronoun_count\n",
    "\n",
    "# Calculate the count of personal pronouns for each article\n",
    "personal_pronoun_counts = []\n",
    "for text in clean['Processed Text']:\n",
    "    personal_pronoun_count = count_personal_pronouns(text)\n",
    "    personal_pronoun_counts.append(personal_pronoun_count)\n",
    "\n",
    "# Add the personal pronoun counts to the DataFrame\n",
    "clean_df['PERSONAL PRONOUNS'] = personal_pronoun_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf1a323",
   "metadata": {},
   "source": [
    "### AVG WORD LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "667e5420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average word length for each article\n",
    "avg_word_lengths = []\n",
    "for text in clean['Processed Text']:\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    # Calculate the total length of all words\n",
    "    total_word_length = sum(len(word) for word in words)\n",
    "    # Calculate the average word length\n",
    "    avg_word_length = total_word_length / len(words)\n",
    "    avg_word_lengths.append(avg_word_length)\n",
    "\n",
    "# Add the average word lengths to the DataFrame\n",
    "clean_df['AVG WORD LENGTH'] = avg_word_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "58fcb217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.459874</td>\n",
       "      <td>23.250000</td>\n",
       "      <td>40.168971</td>\n",
       "      <td>9.634869</td>\n",
       "      <td>36.622642</td>\n",
       "      <td>944</td>\n",
       "      <td>1301</td>\n",
       "      <td>1.801691</td>\n",
       "      <td>1</td>\n",
       "      <td>6.546851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.404217</td>\n",
       "      <td>13.614286</td>\n",
       "      <td>28.436516</td>\n",
       "      <td>5.755053</td>\n",
       "      <td>23.308824</td>\n",
       "      <td>659</td>\n",
       "      <td>952</td>\n",
       "      <td>1.536765</td>\n",
       "      <td>4</td>\n",
       "      <td>5.486884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.477847</td>\n",
       "      <td>17.447761</td>\n",
       "      <td>41.745081</td>\n",
       "      <td>7.311354</td>\n",
       "      <td>27.761194</td>\n",
       "      <td>864</td>\n",
       "      <td>1169</td>\n",
       "      <td>1.863131</td>\n",
       "      <td>2</td>\n",
       "      <td>6.422583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.491477</td>\n",
       "      <td>13.423077</td>\n",
       "      <td>31.136581</td>\n",
       "      <td>5.693204</td>\n",
       "      <td>22.782051</td>\n",
       "      <td>770</td>\n",
       "      <td>1047</td>\n",
       "      <td>1.653295</td>\n",
       "      <td>2</td>\n",
       "      <td>5.741165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.501298</td>\n",
       "      <td>16.178082</td>\n",
       "      <td>32.853514</td>\n",
       "      <td>6.796720</td>\n",
       "      <td>28.367647</td>\n",
       "      <td>836</td>\n",
       "      <td>1177</td>\n",
       "      <td>1.672048</td>\n",
       "      <td>7</td>\n",
       "      <td>5.927180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>146</td>\n",
       "      <td>https://insights.blackcoffer.com/blockchain-fo...</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.399679</td>\n",
       "      <td>15.743590</td>\n",
       "      <td>36.319218</td>\n",
       "      <td>6.644016</td>\n",
       "      <td>25.684211</td>\n",
       "      <td>416</td>\n",
       "      <td>614</td>\n",
       "      <td>1.612378</td>\n",
       "      <td>1</td>\n",
       "      <td>6.596091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>147</td>\n",
       "      <td>https://insights.blackcoffer.com/the-future-of...</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.393396</td>\n",
       "      <td>23.916667</td>\n",
       "      <td>30.749129</td>\n",
       "      <td>9.883740</td>\n",
       "      <td>35.469388</td>\n",
       "      <td>789</td>\n",
       "      <td>1148</td>\n",
       "      <td>1.533972</td>\n",
       "      <td>0</td>\n",
       "      <td>5.781359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>148</td>\n",
       "      <td>https://insights.blackcoffer.com/big-data-anal...</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.395841</td>\n",
       "      <td>13.131148</td>\n",
       "      <td>37.453184</td>\n",
       "      <td>5.593033</td>\n",
       "      <td>20.721311</td>\n",
       "      <td>588</td>\n",
       "      <td>801</td>\n",
       "      <td>1.799001</td>\n",
       "      <td>0</td>\n",
       "      <td>6.041199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>149</td>\n",
       "      <td>https://insights.blackcoffer.com/business-anal...</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.541296</td>\n",
       "      <td>26.444444</td>\n",
       "      <td>47.478992</td>\n",
       "      <td>10.920635</td>\n",
       "      <td>42.166667</td>\n",
       "      <td>342</td>\n",
       "      <td>476</td>\n",
       "      <td>1.941176</td>\n",
       "      <td>0</td>\n",
       "      <td>7.029412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>150</td>\n",
       "      <td>https://insights.blackcoffer.com/challenges-an...</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.446125</td>\n",
       "      <td>11.177419</td>\n",
       "      <td>36.219336</td>\n",
       "      <td>4.810362</td>\n",
       "      <td>18.387097</td>\n",
       "      <td>545</td>\n",
       "      <td>693</td>\n",
       "      <td>1.777778</td>\n",
       "      <td>0</td>\n",
       "      <td>6.011544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID                                                URL  \\\n",
       "0        37  https://insights.blackcoffer.com/ai-in-healthc...   \n",
       "1        38  https://insights.blackcoffer.com/what-if-the-c...   \n",
       "2        39  https://insights.blackcoffer.com/what-jobs-wil...   \n",
       "3        40  https://insights.blackcoffer.com/will-machine-...   \n",
       "4        41  https://insights.blackcoffer.com/will-ai-repla...   \n",
       "..      ...                                                ...   \n",
       "109     146  https://insights.blackcoffer.com/blockchain-fo...   \n",
       "110     147  https://insights.blackcoffer.com/the-future-of...   \n",
       "111     148  https://insights.blackcoffer.com/big-data-anal...   \n",
       "112     149  https://insights.blackcoffer.com/business-anal...   \n",
       "113     150  https://insights.blackcoffer.com/challenges-an...   \n",
       "\n",
       "     POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0             0.166           0.057           0.109            0.459874   \n",
       "1             0.201           0.092           0.109            0.404217   \n",
       "2             0.153           0.079           0.074            0.477847   \n",
       "3             0.228           0.059           0.169            0.491477   \n",
       "4             0.197           0.061           0.136            0.501298   \n",
       "..              ...             ...             ...                 ...   \n",
       "109           0.160           0.071           0.089            0.399679   \n",
       "110           0.134           0.025           0.109            0.393396   \n",
       "111           0.133           0.133           0.000            0.395841   \n",
       "112           0.262           0.007           0.255            0.541296   \n",
       "113           0.242           0.101           0.141            0.446125   \n",
       "\n",
       "     AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0              23.250000                    40.168971   9.634869   \n",
       "1              13.614286                    28.436516   5.755053   \n",
       "2              17.447761                    41.745081   7.311354   \n",
       "3              13.423077                    31.136581   5.693204   \n",
       "4              16.178082                    32.853514   6.796720   \n",
       "..                   ...                          ...        ...   \n",
       "109            15.743590                    36.319218   6.644016   \n",
       "110            23.916667                    30.749129   9.883740   \n",
       "111            13.131148                    37.453184   5.593033   \n",
       "112            26.444444                    47.478992  10.920635   \n",
       "113            11.177419                    36.219336   4.810362   \n",
       "\n",
       "     AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                           36.622642                 944        1301   \n",
       "1                           23.308824                 659         952   \n",
       "2                           27.761194                 864        1169   \n",
       "3                           22.782051                 770        1047   \n",
       "4                           28.367647                 836        1177   \n",
       "..                                ...                 ...         ...   \n",
       "109                         25.684211                 416         614   \n",
       "110                         35.469388                 789        1148   \n",
       "111                         20.721311                 588         801   \n",
       "112                         42.166667                 342         476   \n",
       "113                         18.387097                 545         693   \n",
       "\n",
       "     SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0             1.801691                  1         6.546851  \n",
       "1             1.536765                  4         5.486884  \n",
       "2             1.863131                  2         6.422583  \n",
       "3             1.653295                  2         5.741165  \n",
       "4             1.672048                  7         5.927180  \n",
       "..                 ...                ...              ...  \n",
       "109           1.612378                  1         6.596091  \n",
       "110           1.533972                  0         5.781359  \n",
       "111           1.799001                  0         6.041199  \n",
       "112           1.941176                  0         7.029412  \n",
       "113           1.777778                  0         6.011544  \n",
       "\n",
       "[102 rows x 15 columns]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "1df0bc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv('output.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a206a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
